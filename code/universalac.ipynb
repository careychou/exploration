{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from: \n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "    Story the data with it priority in tree and data frameworks.\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx]:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n",
    "\n",
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
    "        pri_seg = self.tree.total_p / n       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p     # for later calculate ISweight\n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.total_p\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i, :] = idx, data\n",
    "        return b_idx, b_memory, ISWeights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalACN:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            n_weights,\n",
    "            n_skills,\n",
    "            learning_rate=0.0005,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=1,\n",
    "            memory_size=3000,\n",
    "            batch_size=1,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "            double_q=False,\n",
    "            dueling=False,\n",
    "            a2c=False,\n",
    "            prioritized=False,\n",
    "            sess=None       \n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.n_l1 = n_weights\n",
    "        self.n_skills = n_skills\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        self.double_q = double_q    # decide to use double q or not\n",
    "        self.dueling = dueling # dueling DQN\n",
    "        self.a2c = a2c # advantage actor critic\n",
    "        self.prioritized = prioritized    # decide to use double q or not\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # build NN layers\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        if self.prioritized:\n",
    "            self.memory = Memory(capacity=memory_size)\n",
    "        else:\n",
    "            self.memory = np.zeros((self.memory_size, n_features*2+2))\n",
    "            \n",
    "        if sess is None:\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.sess = sess\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "        self.cost_his = []\n",
    "        self.disc_cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        def build_discriminator(s, c_names, n_l1, w_initializer, b_initializer):\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)                              \n",
    "                    \n",
    "            with tf.variable_scope('Discriminator'):                    \n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_skills], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_skills], initializer=b_initializer, collections=c_names)\n",
    "                out = tf.matmul(l1, w2) + b2                         \n",
    "                \n",
    "            return out\n",
    "\n",
    "        def build_layers(s, c_names, n_l1, w_initializer, b_initializer):\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)                              \n",
    "                                \n",
    "            if self.dueling: # Dueling DQN\n",
    "                with tf.variable_scope('Value'):\n",
    "                    w2 = tf.get_variable('w2', [n_l1, 1], initializer=w_initializer, collections=c_names)\n",
    "                    b2 = tf.get_variable('b2', [1, 1], initializer=b_initializer, collections=c_names)\n",
    "                    self.V = tf.matmul(l1, w2) + b2\n",
    "\n",
    "                with tf.variable_scope('Advantage'):\n",
    "                    w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                    b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                    self.A = tf.matmul(l1, w2) + b2           \n",
    "                    \n",
    "                with tf.variable_scope('Q'):     # Q = V(s) + A(s,a)                    \n",
    "                    out = self.V + (self.A - tf.reduce_mean(self.A, axis=1, keep_dims=True)) \n",
    "\n",
    "            elif self.a2c: # A2C\n",
    "                with tf.variable_scope('Critic'):\n",
    "                    w2 = tf.get_variable('w2', [n_l1, 1], initializer=w_initializer, collections=c_names)\n",
    "                    b2 = tf.get_variable('b2', [1, 1], initializer=b_initializer, collections=c_names)\n",
    "                    self.V = tf.matmul(l1, w2) + b2\n",
    "\n",
    "                with tf.variable_scope('Actor'):\n",
    "                    w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                    b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                    self.A = tf.nn.softmax(tf.matmul(l1, w2) + b2)\n",
    "                                                            \n",
    "                    out = (self.V, self.A)\n",
    "                                        \n",
    "            else:\n",
    "                with tf.variable_scope('Q'):\n",
    "                    w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                    b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                    out = tf.matmul(l1, w2) + b2\n",
    "\n",
    "            return out\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_action = tf.placeholder(tf.int32, [None], name='Q_action')  # action by agent\n",
    "        self.q_skill = tf.placeholder(tf.int32, [None], name='Q_skill')  # for z skill\n",
    "        _p_z = np.full(self.n_skills, 1.0 / self.n_skills)\n",
    "        \n",
    "        if self.a2c:\n",
    "            self.q_target = tf.placeholder(tf.float32, [None, 1], name='Q_target')  # for calculating loss                    \n",
    "        else:\n",
    "            self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss                    \n",
    "        \n",
    "        if self.prioritized:\n",
    "            self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')\n",
    "            \n",
    "        with tf.variable_scope('disc_net'):\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['disc_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 20, \\\n",
    "                tf.random_normal_initializer(0., 1), tf.constant_initializer(0.1)  # config of layers\n",
    "            \n",
    "            self.disc = build_discriminator(self.s, c_names, self.n_l1, w_initializer, b_initializer)   \n",
    "            self.disc_z = tf.one_hot(self.q_skill, depth=self.n_skills)            \n",
    "            self.p_z = tf.reduce_sum(_p_z * self.disc_z, axis=1)\n",
    "            self.disc_reward = tf.nn.softmax_cross_entropy_with_logits(labels=self.disc_z, logits=self.disc) - self.p_z\n",
    "            \n",
    "        with tf.variable_scope('eval_net'):\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 20, \\\n",
    "                tf.random_normal_initializer(0., 1), tf.constant_initializer(0.1)  # config of layers\n",
    "                        \n",
    "            self.q_eval = build_layers(self.s, c_names, self.n_l1, w_initializer, b_initializer)            \n",
    "            \n",
    "            if self.a2c:\n",
    "                self.action_probs = self.q_eval[1]\n",
    "                self.q_eval = self.q_eval[0]\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.advantage = self.q_target - self.q_eval            \n",
    "            \n",
    "            if self.a2c:                                \n",
    "                self.entropy_loss = -tf.reduce_sum(self.action_probs * tf.log(self.action_probs + 1e-10), axis=1, keepdims=True)\n",
    "                self.policy_loss = -tf.log(tf.reduce_max(self.action_probs * \n",
    "                                                     tf.squeeze(tf.one_hot(self.q_action, depth=self.n_actions)), \n",
    "                                                     axis=1, keepdims=True) + 1e-10) * self.advantage\n",
    "\n",
    "                self.loss1 = self.policy_loss + 0.5 * self.advantage - 0.01 * self.entropy_loss\n",
    "            else:\n",
    "                self.loss1 = self.advantage\n",
    "            \n",
    "            if self.prioritized:\n",
    "                self.abs_errors = tf.reduce_sum(tf.abs(self.advantage), axis=1)    # for updating Sumtree\n",
    "                self.loss = tf.reduce_mean(self.ISWeights * self.loss1)\n",
    "            else:            \n",
    "                self.loss = tf.reduce_mean(self.loss1)\n",
    "                \n",
    "            self.disc_loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels=self.disc_z, logits=self.disc))\n",
    "            \n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "            self._train_disc_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.disc_loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            self.q_next = build_layers(self.s_, c_names, self.n_l1, w_initializer, b_initializer)\n",
    "            if self.a2c:\n",
    "                self.q_next = self.q_next[0]\n",
    "\n",
    "            \n",
    "    def reset_store(self):\n",
    "        if self.prioritized:\n",
    "            self.memory = Memory(capacity=self.memory_size)\n",
    "        else:\n",
    "            self.memory = np.zeros((self.memory_size, self.n_features*2+2))\n",
    "        \n",
    "        self.memory_counter = 0\n",
    "        \n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if self.prioritized:    # prioritized replay\n",
    "            transition = np.hstack((s, [a, r], s_))\n",
    "            self.memory.store(transition)    # have high priority for newly arrived transition\n",
    "        else:\n",
    "            if not hasattr(self, 'memory_counter'):\n",
    "                self.memory_counter = 0\n",
    "            transition = np.hstack((s, [a, r], s_))\n",
    "            index = self.memory_counter % self.memory_size\n",
    "            self.memory[index, :] = transition\n",
    "            self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = observation[np.newaxis, :]\n",
    "        if self.a2c:            \n",
    "            actions = self.sess.run(self.action_probs, feed_dict={self.s: observation})\n",
    "            action = np.random.choice(self.n_actions, p=actions[0])\n",
    "        else:\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "\n",
    "            if not hasattr(self, 'q'):  # record action value it gets\n",
    "                self.q = []\n",
    "                self.running_q = 0\n",
    "            self.running_q = self.running_q*0.99 + 0.01 * np.max(actions_value)\n",
    "            self.q.append(self.running_q)\n",
    "\n",
    "            if np.random.uniform() > self.epsilon:  # choosing random action\n",
    "                action = np.random.randint(0, self.n_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            #print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        if self.prioritized:\n",
    "            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)\n",
    "        else:\n",
    "            if self.memory_counter > self.memory_size:\n",
    "                sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "            else:\n",
    "                sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "            batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next, q_eval4next = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={self.s_: batch_memory[:, -self.n_features:],    # next observation\n",
    "                       self.s: batch_memory[:, -self.n_features:]})    # next observation\n",
    "        \n",
    "        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.n_features]})        \n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        \n",
    "        self.skills = batch_memory[:, self.n_features + 1]                   \n",
    "        self.reward = self.sess.run(self.disc_reward, feed_dict={\n",
    "                                            self.s: batch_memory[:, :self.n_features], \n",
    "                                            self.q_skill:self.skills})\n",
    "        \n",
    "        if self.double_q:\n",
    "            max_act4next = np.argmax(q_eval4next, axis=1)        # the action that brings the highest value is evaluated by q_eval\n",
    "            selected_q_next = q_next[batch_index, max_act4next]  # Double DQN, select q_next depending on above actions\n",
    "        else:\n",
    "            selected_q_next = np.max(q_next, axis=1)    # the natural DQN\n",
    "\n",
    "        if self.a2c:\n",
    "            q_target[batch_index, 0] = self.reward + self.gamma * selected_q_next\n",
    "        else:\n",
    "            q_target[batch_index, eval_act_index] = self.reward + self.gamma * selected_q_next\n",
    "        \n",
    "        if self.prioritized: # use prioritized replay\n",
    "            _, abs_errors, self.cost = self.sess.run([self._train_op, self.abs_errors, self.loss],\n",
    "                                         feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                    self.q_target: q_target,\n",
    "                                                    self.q_action: eval_act_index,\n",
    "                                                    self.ISWeights: ISWeights})\n",
    "            self.memory.batch_update(tree_idx, abs_errors)     # update priority        \n",
    "        else:\n",
    "            _, self.cost, _, self.disc_cost = self.sess.run([self._train_op, \n",
    "                                                             self.loss, \n",
    "                                                             self._train_disc_op, \n",
    "                                                             self.disc_loss],\n",
    "                                        feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                   self.q_action: eval_act_index,\n",
    "                                                   self.q_target: q_target,\n",
    "                                                   self.q_skill:self.skills})\n",
    "\n",
    "        self.cost_his.append(self.cost)\n",
    "        self.disc_cost_his.append(self.disc_cost)\n",
    "\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "    def reward_info(self, obs, skill):\n",
    "        return self.sess.run([self.disc_reward], \n",
    "                             feed_dict={self.s: obs, self.q_skill:skill})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "rl = UniversalACN(n_actions=1,\n",
    "                  n_features=3,\n",
    "                  n_weights=7,\n",
    "                  n_skills=2, \n",
    "                  batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost= -0.76721114 disc_cost= 0.024358863\n",
      "cost= 16.17832 disc_cost= 16.631063\n",
      "cost= 4.963382 disc_cost= 6.0382285\n",
      "cost= -0.09931046 disc_cost= 2.2649509e-05\n",
      "skill=0 >>>  [array([-0.47505087, -0.49924487, -0.49997735, -0.49999928, -0.5       ,\n",
      "       -0.5       ], dtype=float32)]\n",
      "skill=1 >>>  [array([ 3.2033668,  6.6889606, 10.198015 , 13.70778  , 17.217566 ,\n",
      "       20.727352 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "observation = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5], [6, 6, 6]])\n",
    "\n",
    "rl.reset_store()\n",
    "rl.store_transition(observation[0], rl.choose_action(observation[0]), 0, observation[1])\n",
    "rl.store_transition(observation[1], rl.choose_action(observation[1]), 0, observation[2])\n",
    "rl.store_transition(observation[2], rl.choose_action(observation[2]), 0, observation[1])\n",
    "rl.learn()\n",
    "print(\"cost=\", rl.cost, \"disc_cost=\", rl.disc_cost)\n",
    "\n",
    "rl.reset_store()\n",
    "rl.store_transition(observation[3], rl.choose_action(observation[3]), 1, observation[4])\n",
    "rl.store_transition(observation[4], rl.choose_action(observation[4]), 1, observation[5])\n",
    "rl.store_transition(observation[5], rl.choose_action(observation[5]), 1, observation[4])\n",
    "rl.learn()\n",
    "print(\"cost=\", rl.cost, \"disc_cost=\", rl.disc_cost)\n",
    "\n",
    "rl.reset_store()\n",
    "rl.store_transition(observation[0], rl.choose_action(observation[0]), 1, observation[4])\n",
    "rl.store_transition(observation[4], rl.choose_action(observation[4]), 1, observation[1])\n",
    "rl.store_transition(observation[1], rl.choose_action(observation[1]), 1, observation[2])\n",
    "rl.learn()\n",
    "print(\"cost=\", rl.cost, \"disc_cost=\", rl.disc_cost)\n",
    "\n",
    "rl.reset_store()\n",
    "rl.store_transition(observation[5], rl.choose_action(observation[0]), 0, observation[1])\n",
    "rl.store_transition(observation[1], rl.choose_action(observation[1]), 0, observation[2])\n",
    "rl.store_transition(observation[2], rl.choose_action(observation[2]), 0, observation[1])\n",
    "rl.learn()\n",
    "print(\"cost=\", rl.cost, \"disc_cost=\", rl.disc_cost)\n",
    "\n",
    "print(\"skill=0 >>> \", rl.reward_info(observation, np.repeat(0, len(observation))))\n",
    "print(\"skill=1 >>> \", rl.reward_info(observation, np.repeat(1, len(observation))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "target_params_replaced\n",
      "\n",
      "action: 0\n"
     ]
    }
   ],
   "source": [
    "rl.learn()\n",
    "\n",
    "print(\"action:\", rl.choose_action(observation[1]))\n",
    "\n",
    "#l, l1, a1, p1, e1 = rl.sess.run([rl.loss, rl.loss1, rl.advantage, rl.policy_loss, rl.entropy_loss],\n",
    "#                      feed_dict={rl.q_target: qe2, rl.s:observation, rl.q_action:[0, 2, 0]})\n",
    "\n",
    "#print(\"loss: \", l)\n",
    "#print(\"advantage: \", np.mean(a1))\n",
    "#print(\"policy loss: \", np.mean(p1))\n",
    "#print(\"entropy loss: \", np.mean(e1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
